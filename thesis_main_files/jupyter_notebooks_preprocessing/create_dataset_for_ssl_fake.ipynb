{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62f43301-7d5f-497b-a97b-d6491b4027df",
   "metadata": {},
   "source": [
    "# Import filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada62635-cf51-4dcb-b136-ea3c9813c80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded metadata.csv with shape: (136304, 13)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# ---- Paths ----\n",
    "metadata_path = '/Users/abhishekgupte_macbookpro/PycharmProjects/project_combined_repo_clean/thesis_main_files/datasets/processed/csv_files/lav_df/metadata/metadata.csv'  # Adjust path as needed\n",
    "out_dir = \"/Users/abhishekgupte_macbookpro/PycharmProjects/project_combined_repo_clean_preprocessing/files/csv_files/processed/video\"       # separate folder for this notebook\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# ---- Load CSV ----\n",
    "df = pd.read_csv(metadata_path)\n",
    "print(f\"‚úÖ Loaded metadata.csv with shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5193d388-a5df-4eda-9731-7e11c2376fac",
   "metadata": {},
   "source": [
    "# Index Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76b497f7-9195-4751-b148-c0acbc65acdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Parsed list-like columns: ['fake_periods', 'timestamps']\n"
     ]
    }
   ],
   "source": [
    "list_columns = [\"fake_periods\", \"timestamps\"]\n",
    "\n",
    "def safe_literal_eval(x):\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return x\n",
    "    x = str(x).strip()\n",
    "    if x in (\"\", \"[]\", \"None\", \"nan\"):\n",
    "        return []\n",
    "    try:\n",
    "        return ast.literal_eval(x)\n",
    "    except Exception:\n",
    "        # If parsing fails, return empty list to avoid crashing\n",
    "        return []\n",
    "\n",
    "for col in list_columns:\n",
    "    df[col] = df[col].apply(safe_literal_eval)\n",
    "\n",
    "print(\"‚úÖ Parsed list-like columns:\", list_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74810652-adc2-42ab-8933-54e9d1b306f7",
   "metadata": {},
   "source": [
    "# Ensure modify flags are boolean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4b91296-eea8-4aa0-af37-775e085a4782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dtypes set. Summary:\n",
      "modify_audio       bool\n",
      "modify_video       bool\n",
      "n_fakes           int64\n",
      "duration        float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df[\"modify_audio\"] = df[\"modify_audio\"].astype(bool)\n",
    "\n",
    "# Ensure numeric fields are numeric\n",
    "df[\"n_fakes\"]  = pd.to_numeric(df[\"n_fakes\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "df[\"duration\"] = pd.to_numeric(df[\"duration\"], errors=\"coerce\")\n",
    "\n",
    "print(\"‚úÖ Dtypes set. Summary:\")\n",
    "print(df.dtypes[[\"modify_audio\",\"modify_video\",\"n_fakes\",\"duration\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34384ec-4aca-4818-a8f5-a1b571869e81",
   "metadata": {},
   "source": [
    "# Binary label: 1 = fake (>=1 segment), 0 = real\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e55f3bb7-c9a8-45f4-aa22-703f7b406850",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"] = (df[\"n_fakes\"] >= 1).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f7f393-cb3f-4664-b17c-a7e11a403b56",
   "metadata": {},
   "source": [
    "# A/V combination label e.g. A1_V0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c8ca7f1-c072-4812-ac86-60c8e23d4fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Engineered columns added: label, av_combo, fake_segment_count, total_fake_length\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>n_fakes</th>\n",
       "      <th>label</th>\n",
       "      <th>av_combo</th>\n",
       "      <th>fake_segment_count</th>\n",
       "      <th>total_fake_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A0_V0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000000.mp4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A0_V0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000002.mp4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A1_V1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000003.mp4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A0_V1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000004.mp4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A1_V0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         file  n_fakes  label av_combo  fake_segment_count  total_fake_length\n",
       "0  000001.mp4        0      0    A0_V0                   0              0.000\n",
       "1  000000.mp4        0      0    A0_V0                   0              0.000\n",
       "2  000002.mp4        1      1    A1_V1                   1              0.724\n",
       "3  000003.mp4        1      1    A0_V1                   1              0.280\n",
       "4  000004.mp4        1      1    A1_V0                   1              0.704"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"av_combo\"] = df.apply(lambda r: f\"A{int(r['modify_audio'])}_V{int(r['modify_video'])}\", axis=1)\n",
    "\n",
    "# Count of fake segments from fake_periods\n",
    "df[\"fake_segment_count\"] = df[\"fake_periods\"].apply(lambda x: len(x) if x else 0)\n",
    "\n",
    "# Total fake segment length (seconds)\n",
    "def compute_total_fake_length(fake_periods):\n",
    "    if not fake_periods:\n",
    "        return 0.0\n",
    "    return sum((end - start) for start, end in fake_periods)\n",
    "\n",
    "df[\"total_fake_length\"] = df[\"fake_periods\"].apply(compute_total_fake_length)\n",
    "\n",
    "print(\"‚úÖ Engineered columns added: label, av_combo, fake_segment_count, total_fake_length\")\n",
    "df[[\"file\",\"n_fakes\",\"label\",\"av_combo\",\"fake_segment_count\",\"total_fake_length\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85e3c7c-c9e3-4191-b731-48881a240b1d",
   "metadata": {},
   "source": [
    "# Focus only on rows that contain fake segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb8d36cf-4be3-4d20-b410-a9f8add00d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_fake_row = df[\"n_fakes\"] >= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7a931f-195c-456b-afc1-6326204dd77e",
   "metadata": {},
   "source": [
    "# Modality masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "604fdae3-2bb1-4f71-b326-0d9891fa732c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Masks prepared (fake-only, modality, duration).\n",
      "‚úÖ Saved 6 fake-only subsets to: /Users/abhishekgupte_macbookpro/PycharmProjects/project_combined_repo_clean_preprocessing/files/csv_files/processed/video\n",
      "  A_only_lt7p5: 15578 rows\n",
      "  V_only_lt7p5: 16699 rows\n",
      "  AV_both_lt7p5: 15416 rows\n",
      "  A_only_ge7p5: 17592 rows\n",
      "  V_only_ge7p5: 16844 rows\n",
      "  AV_both_ge7p5: 17744 rows\n",
      "‚úÖ Saved 30% holdouts per subset:\n",
      "  holdout_30pct_A_only_lt7p5: 4673 rows\n",
      "  holdout_30pct_V_only_lt7p5: 5010 rows\n",
      "  holdout_30pct_AV_both_lt7p5: 4625 rows\n",
      "  holdout_30pct_A_only_ge7p5: 5278 rows\n",
      "  holdout_30pct_V_only_ge7p5: 5053 rows\n",
      "  holdout_30pct_AV_both_ge7p5: 5323 rows\n"
     ]
    }
   ],
   "source": [
    "audio_only  = (df[\"modify_audio\"] == True)  & (df[\"modify_video\"] == False)\n",
    "video_only  = (df[\"modify_audio\"] == False) & (df[\"modify_video\"] == True)\n",
    "both_fake   = (df[\"modify_audio\"] == True)  & (df[\"modify_video\"] == True)\n",
    "\n",
    "# Duration threshold and masks\n",
    "thr = 7.5  # seconds\n",
    "short = df[\"duration\"] <  thr\n",
    "long_ = df[\"duration\"] >= thr\n",
    "\n",
    "print(\"‚úÖ Masks prepared (fake-only, modality, duration).\")\n",
    "\n",
    "subsets = {\n",
    "    # < 7.5s\n",
    "    \"A_only_lt7p5\" : df.loc[is_fake_row & audio_only & short, :].copy(),\n",
    "    \"V_only_lt7p5\" : df.loc[is_fake_row & video_only & short, :].copy(),\n",
    "    \"AV_both_lt7p5\": df.loc[is_fake_row & both_fake  & short, :].copy(),\n",
    "    # ‚â• 7.5s\n",
    "    \"A_only_ge7p5\" : df.loc[is_fake_row & audio_only & long_, :].copy(),\n",
    "    \"V_only_ge7p5\" : df.loc[is_fake_row & video_only & long_, :].copy(),\n",
    "    \"AV_both_ge7p5\": df.loc[is_fake_row & both_fake  & long_, :].copy(),\n",
    "}\n",
    "\n",
    "counts = {}\n",
    "for name, dsub in subsets.items():\n",
    "    path = os.path.join(out_dir, f\"{name}.csv\")\n",
    "    dsub.to_csv(path, index=False)  # keep all columns\n",
    "    counts[name] = len(dsub)\n",
    "\n",
    "print(\"‚úÖ Saved 6 fake-only subsets to:\", out_dir)\n",
    "for k, v in counts.items():\n",
    "    print(f\"  {k}: {v} rows\")\n",
    "\n",
    "\n",
    "\n",
    "holdout_counts = {}\n",
    "for name, dsub in subsets.items():\n",
    "    if len(dsub) >= 3:\n",
    "        holdout = dsub.sample(frac=0.3, random_state=42)\n",
    "    elif len(dsub) > 0:\n",
    "        # tiny fallback to avoid empty holdouts\n",
    "        holdout = dsub.sample(n=1, random_state=42)\n",
    "    else:\n",
    "        holdout = dsub  # empty\n",
    "\n",
    "    holdout_path = os.path.join(out_dir, f\"holdout_30pct_{name}.csv\")\n",
    "    holdout.to_csv(holdout_path, index=False)\n",
    "    holdout_counts[name] = len(holdout)\n",
    "\n",
    "print(\"‚úÖ Saved 30% holdouts per subset:\")\n",
    "for k, v in holdout_counts.items():\n",
    "    print(f\"  holdout_30pct_{k}: {v} rows\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fab3433-99ea-4aba-9c57-744f424337bd",
   "metadata": {},
   "source": [
    "# Extracted a total of 7500 for final EVAL of SSL trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366336b2-8089-4732-aac4-0772cc42eaf3",
   "metadata": {},
   "source": [
    "## Import file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec5797f3-ba13-44be-a092-a2c3c334de72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Input root: /Users/abhishekgupte_macbookpro/PycharmProjects/project_combined_repo_clean_preprocessing/files/csv_files/processed/video/fake_files\n",
      "‚úì Evaluate out: /Users/abhishekgupte_macbookpro/PycharmProjects/project_combined_repo_clean_preprocessing/files/csv_files/processed/video/fake_files/evaluate\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# --- Parameters ---\n",
    "input_root_dir = \"/Users/abhishekgupte_macbookpro/PycharmProjects/project_combined_repo_clean_preprocessing/files/csv_files/processed/video/fake_files\"  # CHANGE THIS to your folder with the CSVs\n",
    "eval_subdir = \"evaluate\"\n",
    "sample_size = 7500\n",
    "random_seed = 42\n",
    "\n",
    "# --- Ensure evaluate subfolder exists ---\n",
    "eval_dir = os.path.join(input_root_dir, eval_subdir)\n",
    "os.makedirs(eval_dir, exist_ok=True)\n",
    "\n",
    "print(f\"‚úì Input root: {input_root_dir}\")\n",
    "print(f\"‚úì Evaluate out: {eval_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07566e8-b1e9-472f-a3ba-22ff3275085a",
   "metadata": {},
   "source": [
    "## Sample 7,500 rows from each FAKE CSV subset} and save to evaluate/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6200f55-72fa-4d15-a580-18398227bb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 CSV files.\n",
      "‚úì AV_both_ge7p5.csv: sampled 7500 ‚Üí /Users/abhishekgupte_macbookpro/PycharmProjects/project_combined_repo_clean_preprocessing/files/csv_files/processed/video/fake_files/evaluate/AV_both_ge7p5.csv\n",
      "‚úì AV_both_lt7p5.csv: sampled 7500 ‚Üí /Users/abhishekgupte_macbookpro/PycharmProjects/project_combined_repo_clean_preprocessing/files/csv_files/processed/video/fake_files/evaluate/AV_both_lt7p5.csv\n",
      "‚úì A_only_ge7p5.csv: sampled 7500 ‚Üí /Users/abhishekgupte_macbookpro/PycharmProjects/project_combined_repo_clean_preprocessing/files/csv_files/processed/video/fake_files/evaluate/A_only_ge7p5.csv\n",
      "‚úì A_only_lt7p5.csv: sampled 7500 ‚Üí /Users/abhishekgupte_macbookpro/PycharmProjects/project_combined_repo_clean_preprocessing/files/csv_files/processed/video/fake_files/evaluate/A_only_lt7p5.csv\n",
      "‚úì V_only_ge7p5.csv: sampled 7500 ‚Üí /Users/abhishekgupte_macbookpro/PycharmProjects/project_combined_repo_clean_preprocessing/files/csv_files/processed/video/fake_files/evaluate/V_only_ge7p5.csv\n",
      "‚úì V_only_lt7p5.csv: sampled 7500 ‚Üí /Users/abhishekgupte_macbookpro/PycharmProjects/project_combined_repo_clean_preprocessing/files/csv_files/processed/video/fake_files/evaluate/V_only_lt7p5.csv\n",
      "\n",
      "Summary:\n",
      "  AV_both_ge7p5.csv: 7500 rows saved to evaluate/\n",
      "  AV_both_lt7p5.csv: 7500 rows saved to evaluate/\n",
      "  A_only_ge7p5.csv: 7500 rows saved to evaluate/\n",
      "  A_only_lt7p5.csv: 7500 rows saved to evaluate/\n",
      "  V_only_ge7p5.csv: 7500 rows saved to evaluate/\n",
      "  V_only_lt7p5.csv: 7500 rows saved to evaluate/\n"
     ]
    }
   ],
   "source": [
    "csv_paths = sorted(glob.glob(os.path.join(input_root_dir, \"*.csv\")))\n",
    "if not csv_paths:\n",
    "    print(\"‚ö†Ô∏è No CSV files found in the input root directory.\")\n",
    "else:\n",
    "    print(f\"Found {len(csv_paths)} CSV files.\")\n",
    "\n",
    "summary = []\n",
    "for src_path in csv_paths:\n",
    "    # Skip any CSVs that are already inside evaluate/\n",
    "    if os.path.dirname(src_path) == eval_dir:\n",
    "        continue\n",
    "\n",
    "    fname = os.path.basename(src_path)\n",
    "    dst_path = os.path.join(eval_dir, fname)\n",
    "\n",
    "    # Read source CSV\n",
    "    df_src = pd.read_csv(src_path)\n",
    "\n",
    "    # Sample up to sample_size rows (or all if fewer)\n",
    "    n = min(sample_size, len(df_src))\n",
    "    if n == 0:\n",
    "        print(f\"‚Äî Skipping empty CSV: {fname}\")\n",
    "        summary.append((fname, 0))\n",
    "        continue\n",
    "\n",
    "    df_sampled = df_src.sample(n=n, random_state=random_seed)\n",
    "    df_sampled.to_csv(dst_path, index=False)\n",
    "\n",
    "    print(f\"‚úì {fname}: sampled {n} ‚Üí {dst_path}\")\n",
    "    summary.append((fname, n))\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "for name, n in summary:\n",
    "    print(f\"  {name}: {n} rows saved to evaluate/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d50de9-b84f-434f-8461-2a1b594596f9",
   "metadata": {},
   "source": [
    "## Sample 7,500 rows from each REAL hold-out CSV for each FAKE subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ab344-cde1-485c-8c72-a7d591810943",
   "metadata": {},
   "source": [
    "### Import paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcd12c1d-5e0f-4274-bc86-b3885b3a0dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded REAL holdout: (10929, 17) from:\n",
      "/Users/abhishekgupte_macbookpro/PycharmProjects/project_combined_repo_clean_preprocessing/files/csv_files/processed/video/holdout_30_percent_for_training.csv\n",
      "üìÅ Evaluate directory: /Users/abhishekgupte_macbookpro/PycharmProjects/project_combined_repo_clean_preprocessing/files/csv_files/processed/video/fake_files/evaluate\n",
      "üìÅ Real equivalents will be saved to: /Users/abhishekgupte_macbookpro/PycharmProjects/project_combined_repo_clean_preprocessing/files/csv_files/processed/video/fake_files/evaluate/real_file_equivalent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# --- Inputs ---\n",
    "input_root_dir = \"/Users/abhishekgupte_macbookpro/PycharmProjects/project_combined_repo_clean_preprocessing/files/csv_files/processed/video/fake_files\"  # folder that contains evaluate/\n",
    "evaluate_dir   = os.path.join(input_root_dir, \"evaluate\")\n",
    "real_equiv_dir = os.path.join(evaluate_dir, \"real_file_equivalent\")\n",
    "\n",
    "# Real holdout CSV (ABSOLUTE PATH you provided)\n",
    "real_holdout_path = \"/Users/abhishekgupte_macbookpro/PycharmProjects/project_combined_repo_clean_preprocessing/files/csv_files/processed/video/holdout_30_percent_for_training.csv\"\n",
    "\n",
    "# Sampling settings\n",
    "target_per_file = 7500\n",
    "random_seed     = 42\n",
    "thr             = 7.5  # seconds\n",
    "\n",
    "# --- Ensure output directory exists ---\n",
    "os.makedirs(real_equiv_dir, exist_ok=True)\n",
    "\n",
    "# --- Load REAL holdout once ---\n",
    "df_real_holdout = pd.read_csv(real_holdout_path)\n",
    "df_real_holdout[\"duration\"] = pd.to_numeric(df_real_holdout[\"duration\"], errors=\"coerce\")\n",
    "\n",
    "print(f\"‚úÖ Loaded REAL holdout: {df_real_holdout.shape} from:\\n{real_holdout_path}\")\n",
    "print(f\"üìÅ Evaluate directory: {evaluate_dir}\")\n",
    "print(f\"üìÅ Real equivalents will be saved to: {real_equiv_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa1c6ab-b37f-4552-8abe-ea80b7788a87",
   "metadata": {},
   "source": [
    "### For each evaluate CSV, sample equivalent REAL rows (duration-matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bffa4798-f73e-48f2-a055-1c0b2112ab09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 evaluate CSVs.\n",
      "‚úÖ AV_both_ge7p5.csv: fake=7500, real_sampled=3843 ‚Üí /Users/abhishekgupte_macbookpro/PycharmProjects/project_combined_repo_clean_preprocessing/files/csv_files/processed/video/fake_files/evaluate/real_file_equivalent/AV_both_ge7p5.csv\n",
      "‚úÖ AV_both_lt7p5.csv: fake=7500, real_sampled=7086 ‚Üí /Users/abhishekgupte_macbookpro/PycharmProjects/project_combined_repo_clean_preprocessing/files/csv_files/processed/video/fake_files/evaluate/real_file_equivalent/AV_both_lt7p5.csv\n",
      "‚úÖ A_only_ge7p5.csv: fake=7500, real_sampled=3843 ‚Üí /Users/abhishekgupte_macbookpro/PycharmProjects/project_combined_repo_clean_preprocessing/files/csv_files/processed/video/fake_files/evaluate/real_file_equivalent/A_only_ge7p5.csv\n",
      "‚úÖ A_only_lt7p5.csv: fake=7500, real_sampled=7086 ‚Üí /Users/abhishekgupte_macbookpro/PycharmProjects/project_combined_repo_clean_preprocessing/files/csv_files/processed/video/fake_files/evaluate/real_file_equivalent/A_only_lt7p5.csv\n",
      "‚úÖ V_only_ge7p5.csv: fake=7500, real_sampled=3843 ‚Üí /Users/abhishekgupte_macbookpro/PycharmProjects/project_combined_repo_clean_preprocessing/files/csv_files/processed/video/fake_files/evaluate/real_file_equivalent/V_only_ge7p5.csv\n",
      "‚úÖ V_only_lt7p5.csv: fake=7500, real_sampled=7086 ‚Üí /Users/abhishekgupte_macbookpro/PycharmProjects/project_combined_repo_clean_preprocessing/files/csv_files/processed/video/fake_files/evaluate/real_file_equivalent/V_only_lt7p5.csv\n",
      "\n",
      "üìä Matching summary (per evaluate file):\n",
      "  AV_both_ge7p5.csv                         | fake_rows=  7500 | real_rows=  3843 | tag=ge7p5\n",
      "  AV_both_lt7p5.csv                         | fake_rows=  7500 | real_rows=  7086 | tag=lt7p5\n",
      "  A_only_ge7p5.csv                          | fake_rows=  7500 | real_rows=  3843 | tag=ge7p5\n",
      "  A_only_lt7p5.csv                          | fake_rows=  7500 | real_rows=  7086 | tag=lt7p5\n",
      "  V_only_ge7p5.csv                          | fake_rows=  7500 | real_rows=  3843 | tag=ge7p5\n",
      "  V_only_lt7p5.csv                          | fake_rows=  7500 | real_rows=  7086 | tag=lt7p5\n"
     ]
    }
   ],
   "source": [
    "def pick_duration_mask_from_filename(fname: str):\n",
    "    \"\"\"Return 'lt7p5' or 'ge7p5' if detectable from filename, else None.\"\"\"\n",
    "    low = \"_lt7p5\" in fname.lower()\n",
    "    high = \"_ge7p5\" in fname.lower()\n",
    "    if low and not high:\n",
    "        return \"lt7p5\"\n",
    "    if high and not low:\n",
    "        return \"ge7p5\"\n",
    "    return None\n",
    "\n",
    "# Gather all evaluate CSVs (skip any in real_file_equivalent/)\n",
    "eval_csvs = sorted(glob.glob(os.path.join(evaluate_dir, \"*.csv\")))\n",
    "eval_csvs = [p for p in eval_csvs if os.path.dirname(p) != real_equiv_dir]\n",
    "\n",
    "if not eval_csvs:\n",
    "    print(\"‚ö†Ô∏è No evaluate CSVs found. Make sure you've created samples in the 'evaluate/' folder first.\")\n",
    "else:\n",
    "    print(f\"Found {len(eval_csvs)} evaluate CSVs.\")\n",
    "\n",
    "summary = []\n",
    "for eval_path in eval_csvs:\n",
    "    fname = os.path.basename(eval_path)\n",
    "    real_out_path = os.path.join(real_equiv_dir, fname)\n",
    "\n",
    "    # Read current evaluate CSV (FAKE subset)\n",
    "    df_eval = pd.read_csv(eval_path)\n",
    "    if df_eval.empty:\n",
    "        print(f\"‚Äî Skipping empty evaluate file: {fname}\")\n",
    "        summary.append((fname, 0, 0, \"empty_eval\"))\n",
    "        continue\n",
    "\n",
    "    # Determine duration bucket (prefer filename tag; fallback to data-driven inference)\n",
    "    tag = pick_duration_mask_from_filename(fname)\n",
    "    if tag is None:\n",
    "        # Fallback: infer by majority of durations in fake sample\n",
    "        df_eval[\"duration\"] = pd.to_numeric(df_eval[\"duration\"], errors=\"coerce\")\n",
    "        share_lt = (df_eval[\"duration\"] < thr).mean()\n",
    "        tag = \"lt7p5\" if share_lt >= 0.5 else \"ge7p5\"\n",
    "        print(f\"‚ÑπÔ∏è  Inferred duration tag for {fname} by data: {tag} (lt-share={share_lt:.2f})\")\n",
    "\n",
    "    # Build duration-matched pool from REAL holdout\n",
    "    if tag == \"lt7p5\":\n",
    "        df_real_pool = df_real_holdout[df_real_holdout[\"duration\"] < thr]\n",
    "    else:\n",
    "        df_real_pool = df_real_holdout[df_real_holdout[\"duration\"] >= thr]\n",
    "\n",
    "    # Determine how many rows to sample: match evaluate count up to 7,500, but not exceeding real pool size\n",
    "    n_eval = len(df_eval)\n",
    "    n_target = min(target_per_file, n_eval, len(df_real_pool))\n",
    "\n",
    "    if n_target == 0:\n",
    "        print(f\"‚ö†Ô∏è Not enough REAL rows to match for {fname} with tag {tag}. Skipping.\")\n",
    "        summary.append((fname, n_eval, 0, \"no_real_available\"))\n",
    "        continue\n",
    "\n",
    "    df_real_sample = df_real_pool.sample(n=n_target, random_state=42)\n",
    "    df_real_sample.to_csv(real_out_path, index=False)\n",
    "\n",
    "    print(f\"‚úÖ {fname}: fake={n_eval}, real_sampled={n_target} ‚Üí {real_out_path}\")\n",
    "    summary.append((fname, n_eval, n_target, tag))\n",
    "\n",
    "# Summary\n",
    "print(\"\\nüìä Matching summary (per evaluate file):\")\n",
    "for fname, n_eval, n_real, tag in summary:\n",
    "    print(f\"  {fname:40s}  | fake_rows={n_eval:6d} | real_rows={n_real:6d} | tag={tag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2396f7-a456-47d0-94c8-fe73ccb33e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
